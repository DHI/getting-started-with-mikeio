{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic dfs processing\n",
    "Tools and methods that applies to any type of dfs files. \n",
    "\n",
    "The generic tools are useful for common data processing tasks, where detailed configuration is not necessary. \n",
    "\n",
    "mikeio.generic: methods that read any dfs file and outputs a new dfs file of the same type\n",
    "\n",
    "- concat: Concatenates files along the time axis  \n",
    "- scale: Apply scaling to any dfs file\n",
    "- sum: Sum two dfs files \n",
    "- diff: Calculate difference between two dfs files\n",
    "- extract: Extract timesteps and/or items to a new dfs file\n",
    "- time-avg: Create a temporally averaged dfs file\n",
    "- quantile: Create temporal quantiles of dfs file\n",
    "\n",
    "The generic methods works on larger-than-memory files as they process one time step at a time. This can however make them in-efficient for dfs0 processing! \n",
    "\n",
    "See [Generic in MIKE IO Documentation](https://dhi.github.io/mikeio/generic.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mikeio\n",
    "import mikeio.generic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at these two files with overlapping timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = mikeio.read(\"data/tide1.dfs1\")\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = mikeio.read(\"data/tide2.dfs1\")\n",
    "t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot one of the points along the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = t1[0].isel(x=1).plot(lw=5)\n",
    "t2[0].isel(x=1).plot(ax=ax);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mikeio.generic.concat(infilenames=[\"data/tide1.dfs1\",\n",
    "                                   \"data/tide2.dfs1\"],\n",
    "                      outfilename=\"concat.dfs1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = mikeio.read(\"concat.dfs1\")\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[0].isel(x=1).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract time steps or items\n",
    "\n",
    "The extract() method can extract a part of a file:\n",
    "\n",
    "* **time slice** by specifying *start* and/or *end*\n",
    "* specific **items**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"data/tide1.dfs1\"\n",
    "mikeio.generic.extract(infile, \"extracted.dfs1\", start='2019-01-02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = mikeio.read(\"extracted.dfs1\")\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"data/oresund_vertical_slice.dfsu\"\n",
    "mikeio.generic.extract(infile, \"extracted.dfsu\", items='Salinity', end=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = mikeio.read(\"extracted.dfsu\")\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline exercise**\n",
    "\n",
    "1. use `mikeio.generic.extract` to extract the data from the beginning of the file 'data/tide2.dfs1' until \"2019-01-03\", into a new file named 'tide_start.dfs1'\n",
    "2. use `mikeio.read` to read the new file 'tide_start.dfs1' into a Dataset named ds\n",
    "3. Check that the end time, `ds.end_time` matches what you specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mikeio.read(\"data/tide2.dfs1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diff\n",
    "\n",
    "Take difference between two dfs files with same structure - e.g. to see the difference in result between two calibration runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn1 = \"data/oresundHD_run1.dfsu\"\n",
    "fn2 = \"data/oresundHD_run2.dfsu\"\n",
    "fn_diff = \"oresundHD_difference.dfsu\"\n",
    "mikeio.generic.diff(fn1, fn2, fn_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open the files and visualize the last time step of the first item (water level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,3, sharey=True, figsize=(12,5))\n",
    "da = mikeio.read(fn1, time=-1)[0]\n",
    "da.plot(vmin=0.06, vmax=0.27, ax=ax[0], title='run 1')\n",
    "da = mikeio.read(fn2, time=-1)[0]\n",
    "da.plot(vmin=0.06, vmax=0.27, ax=ax[1], title='run 2')\n",
    "da = mikeio.read(fn_diff, time=-1)[0]\n",
    "da.plot(vmin=-0.1, vmax=0.1, cmap='coolwarm', ax=ax[2], title='difference');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "Adding a constant e.g to adjust datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = mikeio.read(\"data/gebco_sound.dfs2\")\n",
    "ds.Elevation.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['Elevation'][0,104,131]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mikeio.generic.scale(\"data/gebco_sound.dfs2\",\"gebco_sound_local_datum.dfs2\",offset=-2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = mikeio.read(\"gebco_sound_local_datum.dfs2\")\n",
    "ds.Elevation.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2['Elevation'][0,104,131]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatially varying correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "factor = np.ones_like(ds['Elevation'][0].to_numpy())\n",
    "factor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some spatially varying factors, exaggerated values for educational purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor[:,0:100] = 5.3\n",
    "factor[0:40,] = 0.1\n",
    "factor[150:,150:] = 10.7\n",
    "plt.imshow(factor)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2d array must first be flipped upside down and then converted to a 1d vector using [numpy.ndarray.flatten](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.flatten.html) to match how data is stored in dfs files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_ud = np.flipud(factor)\n",
    "factor_vec  = factor_ud.flatten()\n",
    "mikeio.generic.scale(\"data/gebco_sound.dfs2\",\"gebco_sound_spatial.dfs2\",factor=factor_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3 = mikeio.read(\"gebco_sound_spatial.dfs2\")\n",
    "ds3.Elevation.plot()\n",
    "plt.title(\"Spatial correction applied to dfs2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove(\"concat.dfs1\")\n",
    "os.remove(\"extracted.dfs1\")\n",
    "os.remove(\"extracted.dfsu\")\n",
    "os.remove(\"oresundHD_difference.dfsu\")\n",
    "os.remove(\"gebco_sound_local_datum.dfs2\")\n",
    "os.remove(\"gebco_sound_spatial.dfs2\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce8098af3ce22f00283f2dbf9dff06733072d21f076b7f034380a2cf9868eeaa"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
