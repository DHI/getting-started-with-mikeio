{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final assignment: Data processing\n",
    "\n",
    "The final exercise involves converting data from an hypothetical external provider.\n",
    "\n",
    "Parts of this assignment can be solved in several ways. Use descriptive variable names and comments or descriptive text if necessary to clarify.\n",
    "The final solution should be clear to your colleagues and will be shared with some of your colleagues for feedback.\n",
    "\n",
    "The data will be used for MIKE modelling and must be converted to Dfs with apppropriate EUM types/units in order to be used by the MIKE software.\n",
    "\n",
    "The data is provided as a [zip file](https://github.com/DHI/getting-started-with-mikeio/raw/main/mini_book/data/stream_data.zip) and two binary files (see G.3).\n",
    "\n",
    "Inside the zip file, there are a many timeseries of discharge data from streams located across several regions (`*.dat`).\n",
    "\n",
    "Static data for each region is found in a separate file (`region_info.csv`)\n",
    "\n",
    "![](../images/region_info.png)\n",
    "\n",
    "Pandas `read_csv` is very powerful, but here are a few things to keep in mind\n",
    "\n",
    "* Column separator e.g. comma (,)\n",
    "* Blank lines\n",
    "* Comments\n",
    "* Missing values\n",
    "* Date format\n",
    "\n",
    "The MIKE engine can not handle missing values / delete values, fill in in missing values with interpolated values.\n",
    "\n",
    "In order to save diskspace, crop the timeseries to simulation period Feb 1 - June 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G.1 Convert all timeseries to Dfs0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mikeio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is one way to find and filter filenames in a directory\n",
    "# [x for x in os.listdir(\"datafolder\") if \"some_str\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is useful!\n",
    "# help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of reading csv\n",
    "# df = pd.read_csv(\"../data/oceandata.csv\", comment='#', index_col=0, sep=',', parse_dates=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G.2 Add region specific info to normalize timeseries with surface area\n",
    "\n",
    "Each timeseries belongs to a region identified in the filename, e.g. `s15_east_novayork_river.dat` is located in the `novayork` region.\n",
    "\n",
    "For each timeseries in the dataset:\n",
    "1. Find out which region it belongs to\n",
    "2. Divide the timeseries values with the surface area for the region (take into account units)\n",
    "3. Create two dfs0 files, one with discharge and another one with specific discharge *(discharge / area)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G.3 Gridded weather forcing data\n",
    "\n",
    "The dataset is provided in NumPy binary format and consists of\n",
    "\n",
    "* Temperature 2m (degree Kelvin)\n",
    "* Relative humidity 2m (%)\n",
    "\n",
    "The spatial grid is: 40E - 50E, 10-15N with a grid spacing of 1 degree in each direction.\n",
    "\n",
    "The time axis consists of two timesteps '2005-01-31', '2005-07-31' which is sufficent to cover the simulation period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.load(\"../data/temperature_2m.npy\")\n",
    "rh = np.load(\"../data/rel_hum_2m.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy = 1.0\n",
    "dx = 1.0\n",
    "time = pd.date_range(\"2005-01-01\", freq='6M', periods=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mikeio import Dfs2, Dataset\n",
    "from mikeio.eum import ItemInfo, EUMType, EUMUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [tmp, rh]\n",
    "# ds = Dataset(data,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected outcome\n",
    "\n",
    "![](../images/weather_dfs2.png)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa576ebcd40e010bdc0ae86b06ce09151f3424f9e9aed6893ff04f39a9299d89"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
