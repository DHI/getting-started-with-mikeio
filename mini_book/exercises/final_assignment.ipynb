{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Final assignment: Data processing\n",
    "\n",
    "The final exercise involves converting data from one or more providers.\n",
    "\n",
    "Since this exercise is designed to prepare you for real project work, the information you need to solve it might be slightly incomplete or not provided in context. Use your best judgment and most likely your client and your colleagues will be happy ;-)\n",
    "\n",
    "Parts of this assignment can be solved in several ways. Use descriptive variable names and comments or descriptive text if necessary to clarify.\n",
    "The final solution should be clear to your colleagues and will be shared with some of your fellow students for review.\n",
    "\n",
    "The data will be used for MIKE modelling and must be converted to Dfs with apppropriate EUM types/units in order to be used by the MIKE software.\n",
    "\n",
    "The data is provided as a [zip file](https://github.com/DHI/getting-started-with-mikeio/raw/main/mini_book/data/stream_data.zip) and two binary files ([](gridded-data)).\n",
    "\n",
    "Inside the zip file, there are a many timeseries (ASCII format) of discharge data from streams located across several regions (`*.dat`).\n",
    "\n",
    "Static data for each region is found in a separate file (`region_info.csv`)\n",
    "\n",
    "![](../images/region_info.png)\n",
    "\n",
    "Pandas `read_csv` is very powerful, but here are a few things to keep in mind\n",
    "\n",
    "* Column separator e.g. comma (,)\n",
    "* Blank lines\n",
    "* Comments\n",
    "* Missing values\n",
    "* Date format\n",
    "\n",
    "The MIKE engine can not handle missing values / delete values, fill in in missing values with interpolated values.\n",
    "\n",
    "In order to save diskspace, crop the timeseries to simulation period Feb 1 - June 30."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FA.1 Convert all timeseries to Dfs0"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mikeio"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# This is one way to find and filter filenames in a directory\n",
    "# [x for x in os.listdir(\"datafolder\") if \"some_str\" in x]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# This is useful!\n",
    "# help(pd.read_csv)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# example of reading csv\n",
    "# df = pd.read_csv(\"../data/oceandata.csv\", comment='#', index_col=0, sep=',', parse_dates=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FA.2 Add region specific info to normalize timeseries with surface area\n",
    "\n",
    "Each timeseries belongs to a region identified in the filename, e.g. `s15_east_novayork_river.dat` is located in the `novayork` region.\n",
    "\n",
    "For each timeseries in the dataset:\n",
    "1. Find out which region it belongs to\n",
    "2. Divide the timeseries values with the surface area for the region (take into account units)\n",
    "3. Create two dfs0 files, one with discharge and another one with specific discharge *(discharge / area)*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(gridded-data)=\n",
    "## FA.3 Gridded weather forcing data\n",
    "\n",
    "The dataset is provided in NumPy binary format and consists of\n",
    "\n",
    "* Temperature 2m (degree Kelvin)\n",
    "* Relative humidity 2m (%)\n",
    "\n",
    "The spatial grid is: 40E - 50E, 10-15N with a grid spacing of 1 degree in each direction.\n",
    "\n",
    "The time axis consists of two timesteps '2005-01-31', '2005-07-31' which is sufficent to cover the simulation period."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tmp = np.load(\"../data/temperature_2m.npy\")\n",
    "rh = np.load(\"../data/rel_hum_2m.npy\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "dy = 1.0\n",
    "dx = 1.0\n",
    "time = pd.date_range(\"2005-01-01\", freq='6M', periods=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from mikeio import Dfs2, Dataset\n",
    "from mikeio.eum import ItemInfo, EUMType, EUMUnit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "data = [tmp, rh]\n",
    "# ds = Dataset(data,..."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The expected outcome\n",
    "\n",
    "![](../images/weather_dfs2.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Submission of solution\n",
    "\n",
    "Your solution to the above tasks is to be delivered in the format of a single Jupyter notebook file.\n",
    "\n",
    "The solution will be reviewed by a couple of your fellow students, which will provide feedback on both the correctnes and clarity of your solution.\n",
    "\n",
    "The submission and review process is handled by Campus + Eduflow.\n",
    "\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa576ebcd40e010bdc0ae86b06ce09151f3424f9e9aed6893ff04f39a9299d89"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}